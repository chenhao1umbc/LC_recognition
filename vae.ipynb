{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load dependants \n",
    "from utils import *\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from models import VAE, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train VAE with augmented data\n",
    "from utils import *\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from models import VAE, MLP\n",
    "\n",
    "#%% train with aug data\n",
    "tr_pos = torch.load('./data/aug_pos.pt') # data is not normalized\n",
    "tr_neg = torch.load('./data/aug_neg.pt')\n",
    "tr_neg = tr_neg/(tr_neg.abs().amax(dim=(1,2,3,4), keepdim=True) + 1e-5)\n",
    "tr_pos = tr_pos/(tr_pos.abs().amax(dim=(1,2,3,4), keepdim=True) + 1e-5)\n",
    "\n",
    "n_tr = int(tr_neg.shape[0]*0.85)\n",
    "d_tr = torch.cat((tr_neg[:n_tr].reshape(-1, 8, 32, 32) \\\n",
    "                  , tr_pos[:n_tr].reshape(-1, 8, 32, 32)))\n",
    "l_tr = torch.cat((torch.zeros(n_tr*16), torch.ones(n_tr*16)))\n",
    "data = Data.TensorDataset(d_tr, l_tr)\n",
    "tr = Data.DataLoader(data, batch_size=64, shuffle=True)\n",
    "\n",
    "n_val = tr_neg.shape[0]- n_tr\n",
    "d_val = torch.cat((tr_neg[n_tr:].reshape(-1, 8, 32, 32),\\\n",
    "                    tr_pos[n_tr:].reshape(-1, 8, 32, 32)))\n",
    "l_val = torch.cat((torch.zeros(n_val*16), torch.ones(n_val*16)))\n",
    "data = Data.TensorDataset(d_val, l_val)\n",
    "val = Data.DataLoader(data, batch_size=64, shuffle=False)\n",
    "\n",
    "vae = VAE().cuda()\n",
    "mlp = MLP().cuda()\n",
    "opt1 = torch.optim.RAdam(vae.parameters(), lr=1e-4)\n",
    "opt2 = torch.optim.RAdam(mlp.parameters(), lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "tr_loss, val_loss, acc_all = [], [], []\n",
    "for epoch in range(201):\n",
    "    vae.train()\n",
    "    mlp.train()\n",
    "    temp = []\n",
    "    for i, (x,y) in enumerate(tr):\n",
    "        opt1.zero_grad()\n",
    "        opt2.zero_grad()\n",
    "        x_cuda, y_cuda = x.cuda(), y.cuda()\n",
    "        x_hat, mu, logvar, sources = vae(x_cuda)\n",
    "        latent = torch.cat((mu, logvar), dim = 1)\n",
    "        y_hat = mlp(latent)\n",
    "        loss = loss_func(y_hat, y_cuda.to(torch.long)) \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=10)\n",
    "        torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=10)\n",
    "        opt1.step()\n",
    "        opt2.step()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        temp.append(loss.cpu().item()/x.shape[0])\n",
    "    tr_loss.append(sum(temp)/len(temp))\n",
    "\n",
    "    #validation\n",
    "    vae.eval()\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        temp, acc = [], []\n",
    "        for i, (x,y) in enumerate(val):\n",
    "            x_cuda, y_cuda = x.cuda(), y.cuda()\n",
    "            x_hat, mu, logvar, sources = vae(x_cuda)\n",
    "            latent = torch.cat((mu, logvar), dim = 1)\n",
    "            y_hat = mlp(latent)\n",
    "            loss = loss_func(y_hat, y_cuda.to(torch.long)) \n",
    "            temp.append(loss.cpu().item()/x.shape[0])\n",
    "            counter = ((y_hat.argmax(dim=-1)- y_cuda) == 0).sum()\n",
    "            acc.append(counter)\n",
    "\n",
    "        val_loss.append(sum(temp)/len(temp))\n",
    "        acc_all.append(sum(acc)/d_val.shape[0])\n",
    "    print(f'acc at epoch {epoch}:', acc_all[-1])\n",
    "    if acc_all[-1] == max(acc_all):\n",
    "        torch.save((vae, mlp), './res/vae/best_vae_aug.pt')\n",
    "\n",
    "    if epoch%20 == 0 and epoch > 50:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(tr_loss, '-o')\n",
    "        plt.plot(val_loss, '--^')\n",
    "        plt.legend(['Training', 'Validation'])\n",
    "        plt.title(f'Loss fuction at epoch {epoch}')\n",
    "        plt.savefig('./res/vae/LossFun_aug.png')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(tr_loss[-50:], '-o')\n",
    "        plt.plot(val_loss[-50:], '--^')\n",
    "        plt.legend(['Training', 'Validation'])\n",
    "        plt.title(f'Last 50 loss function values at {epoch}')\n",
    "        plt.savefig(f'./res/vae/Last_50 at {epoch}_aug.png')\n",
    "        plt.close('all')\n",
    "\n",
    "        torch.save([tr_loss, val_loss], './res/tr_val_loss_aug.pt')\n",
    "        torch.save((vae, mlp), f'./res/vae/model_epoch{epoch}_aug.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train VAE with augmented data and first 50 epochs not update VAE\n",
    "from utils import *\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from models import VAE, MLP\n",
    "\n",
    "#%% train with aug data\n",
    "tr_pos = torch.load('./data/aug_pos.pt') # data is not normalized\n",
    "tr_neg = torch.load('./data/aug_neg.pt')\n",
    "tr_neg = tr_neg/(tr_neg.abs().amax(dim=(1,2,3,4), keepdim=True) + 1e-5)\n",
    "tr_pos = tr_pos/(tr_pos.abs().amax(dim=(1,2,3,4), keepdim=True) + 1e-5)\n",
    "\n",
    "n_tr = int(tr_neg.shape[0]*0.85)\n",
    "d_tr = torch.cat((tr_neg[:n_tr].reshape(-1, 8, 32, 32) \\\n",
    "                  , tr_pos[:n_tr].reshape(-1, 8, 32, 32)))\n",
    "l_tr = torch.cat((torch.zeros(n_tr*16), torch.ones(n_tr*16)))\n",
    "data = Data.TensorDataset(d_tr, l_tr)\n",
    "tr = Data.DataLoader(data, batch_size=64, shuffle=True)\n",
    "\n",
    "n_val = tr_neg.shape[0]- n_tr\n",
    "d_val = torch.cat((tr_neg[n_tr:].reshape(-1, 8, 32, 32),\\\n",
    "                    tr_pos[n_tr:].reshape(-1, 8, 32, 32)))\n",
    "l_val = torch.cat((torch.zeros(n_val*16), torch.ones(n_val*16)))\n",
    "data = Data.TensorDataset(d_val, l_val)\n",
    "val = Data.DataLoader(data, batch_size=64, shuffle=False)\n",
    "\n",
    "vae = VAE().cuda()\n",
    "mlp = MLP().cuda()\n",
    "opt1 = torch.optim.RAdam(vae.parameters(), lr=1e-4)\n",
    "opt2 = torch.optim.RAdam(mlp.parameters(), lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "tr_loss, val_loss, acc_all = [], [], []\n",
    "for epoch in range(201):\n",
    "    vae.train()\n",
    "    mlp.train()\n",
    "    temp = []\n",
    "    for i, (x,y) in enumerate(tr):\n",
    "        opt1.zero_grad()\n",
    "        opt2.zero_grad()\n",
    "        x_cuda, y_cuda = x.cuda(), y.cuda()\n",
    "        if i <= 50:\n",
    "            with torch.no_grad():\n",
    "                x_hat, mu, logvar, sources = vae(x_cuda)\n",
    "        else:\n",
    "            x_hat, mu, logvar, sources = vae(x_cuda)\n",
    "        latent = torch.cat((mu, logvar), dim = 1)\n",
    "        y_hat = mlp(latent)\n",
    "        loss = loss_func(y_hat, y_cuda.to(torch.long)) \n",
    "        loss.backward()\n",
    "        if i <= 50:\n",
    "            with torch.no_grad():\n",
    "                torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=10)\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=10)\n",
    "        torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=10)\n",
    "        opt1.step()\n",
    "        opt2.step()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        temp.append(loss.cpu().item()/x.shape[0])\n",
    "    tr_loss.append(sum(temp)/len(temp))\n",
    "\n",
    "    #validation\n",
    "    vae.eval()\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        temp, acc = [], []\n",
    "        for i, (x,y) in enumerate(val):\n",
    "            x_cuda, y_cuda = x.cuda(), y.cuda()\n",
    "            x_hat, mu, logvar, sources = vae(x_cuda)\n",
    "            latent = torch.cat((mu, logvar), dim = 1)\n",
    "            y_hat = mlp(latent)\n",
    "            loss = loss_func(y_hat, y_cuda.to(torch.long)) \n",
    "            temp.append(loss.cpu().item()/x.shape[0])\n",
    "            counter = ((y_hat.argmax(dim=-1)- y_cuda) == 0).sum()\n",
    "            acc.append(counter)\n",
    "\n",
    "        val_loss.append(sum(temp)/len(temp))\n",
    "        acc_all.append(sum(acc)/d_val.shape[0])\n",
    "    print(f'acc at epoch {epoch}:', acc_all[-1])\n",
    "    if acc_all[-1] == max(acc_all):\n",
    "        torch.save((vae, mlp), './res/vae/best_vae_aug50.pt')\n",
    "\n",
    "    if epoch%20 == 0 and epoch > 50:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(tr_loss, '-o')\n",
    "        plt.plot(val_loss, '--^')\n",
    "        plt.legend(['Training', 'Validation'])\n",
    "        plt.title(f'Loss fuction at epoch {epoch}')\n",
    "        plt.savefig('./res/vae/LossFun_aug50.png')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(tr_loss[-50:], '-o')\n",
    "        plt.plot(val_loss[-50:], '--^')\n",
    "        plt.legend(['Training', 'Validation'])\n",
    "        plt.title(f'Last 50 loss function values at {epoch}')\n",
    "        plt.savefig(f'./res/vae/Last_50 at {epoch}_aug50.png')\n",
    "        plt.close('all')\n",
    "\n",
    "        torch.save([tr_loss, val_loss], './res/tr_val_loss_aug50.pt')\n",
    "        torch.save((vae, mlp), f'./res/vae/model_epoch{epoch}_aug50.pt')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
